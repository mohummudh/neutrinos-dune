{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Welcome to this notebook to write a simple CNN to classify some standard image datasets. We will make use of `tensorflow` and its high-level API `keras`. You could also do this in other frameworks such as a `PyTorch` if you wanted to.\n",
        "\n",
        "Aims:\n",
        "\n",
        "\n",
        "1.   Write your first CNN using `keras`\n",
        "2.   Consider how to make the model more complex and adapt it to different tasks\n",
        "3.   See how different layers change the number of parameters in the model\n",
        "\n",
        "\n",
        "At various points in the code you will come across the word `None`. Whilst this is actually a keyword in python, all instances in this notebook should be replaced by the actual code required. Clues will be given with the names of functions that you will need to use.\n",
        "\n"
      ],
      "metadata": {
        "id": "Rhj4B-MhPMwt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WU_GuShCusOz"
      },
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "from keras.datasets import mnist, cifar10, cifar100\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plot\n",
        "\n",
        "print('Tensorflow version:',tensorflow.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "You can use the function below to load some of the simple datasets available directly from `keras`. There are three options for the `dataset_name` argument:\n",
        "1. `mnist`: the dataset used for the original CNN paper, consisting of handwritten digits from 0 - 9. These images are (28,28,1) in shape\n",
        "2. `cifar10`: these are small colour images with shape (32,32,3) from ten different classes (plane, car, bird, cat, deer, dog, frog, horse, ship, truck)\n",
        "3. `cifar100`: as above but with 100 classes! This will likely not be feasible to use with just a CPU as it would take a fairly complex network with many parameters. I have included it in case you want to play with this on a GPU one day"
      ],
      "metadata": {
        "id": "uBXkX4Ki2UoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(dataset_name='mnist'):\n",
        "  # MNIST, CIFAR10 and CIFAR100 are standard datasets we can load straight\n",
        "  # from keras. The data are split between train and test sets automatically\n",
        "  # - x_train is a numpy array that stores the training images\n",
        "  # - y_train is a numpy array that stores the true class of the training images\n",
        "  # - x_train is a numpy array that stores the testing images\n",
        "  # - y_train is a numpy array that stores the true class of the testing images\n",
        "  if dataset_name.lower() == 'cifar10':\n",
        "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "    n_classes = 10\n",
        "  elif dataset_name.lower() == 'cifar100':\n",
        "    (x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
        "    n_classes = 100\n",
        "  elif dataset_name.lower() == 'mnist':\n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "    # MNIST is greyscale so we have to do a trick to add a depth dimension\n",
        "    x_train = np.expand_dims(x_train, axis=-1)\n",
        "    x_test = np.expand_dims(x_test, axis=-1)\n",
        "    n_classes = 10\n",
        "  else:\n",
        "    print('Requested dataset does not exist. Please choose from mnist, cifar10 or cifar100')\n",
        "    return\n",
        "\n",
        "  # Let's check the shape of the images for convenience\n",
        "  print(\"Shape of x_train =\",x_train.shape)\n",
        "  print(\"Shape of x_test =\",x_test.shape)\n",
        "\n",
        "  # The y_train and y_test values we loaded also need to be modified.\n",
        "  # These values store the true classification of the images (0-9) as a single\n",
        "  # number. We need to convert the single value into an array of length 10\n",
        "  # corresponding to the number of output classes. Thus values of\n",
        "  # y = 2 becomes y = [0,0,1,0,0,0,0,0,0,0]\n",
        "  # y = 8 becomes y = [0,0,0,0,0,0,0,0,1,0]\n",
        "  y_train = keras.utils.to_categorical(y_train, n_classes)\n",
        "  y_test = keras.utils.to_categorical(y_test, n_classes)\n",
        "\n",
        "  print(\"Shape of y_train =\", y_train.shape)\n",
        "  print(\"Shape of y_test =\", y_test.shape)\n",
        "\n",
        "  # Let's take a look at a few example images from the training set\n",
        "  n_plots=5\n",
        "  fig, ax = plot.subplots(1, n_plots)\n",
        "  for plot_number in range (0, n_plots):\n",
        "    ax[plot_number].imshow(x_train[plot_number])\n",
        "\n",
        "  return (x_train, y_train), (x_test, y_test), n_classes"
      ],
      "metadata": {
        "id": "jVS-09BUvC2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Here we use the `load_data` function to load our dataset. In the\n",
        "first instance we will use `mnist` since it is the simplest dataset and we can use a very simple CNN. Later on you can change this to `cifar10` to see how the performance differs."
      ],
      "metadata": {
        "id": "CpW7YAi4LEo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the input data.\n",
        "# x_train is the training data, and y_train the corresponding true labels\n",
        "# x_test is the testing data, and y_test the corresponding true labels\n",
        "# We don't have a separate validation sample in these keras datasets\n",
        "# Num_classes is the number of true classes\n",
        "(x_train, y_train), (x_test, y_test), num_classes = load_dataset('mnist')"
      ],
      "metadata": {
        "id": "4-x32SkYvHVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Now we want to define a CNN. The basic building blocks you will need are:\n",
        "\n",
        "\n",
        "*   Convolutional layers: `keras.layers.Conv2D(num_filters, (k,k), activation='relu')`. Typical values of `k` are 3, 5, or 7\n",
        "*   Pooling layers: `keras.layers.MaxPooling2D((2,2))` will perform a factor of 2 downsampling in the two dimensions of image\n",
        "*   Dropout: keras.layers.Dropout(fraction) where fraction is the fraction of weights that are ignored. Typical values can be 0.25 or 0.5\n",
        "*   Dense layers: `keras.layers.Dense(num_nodes, activation='relu')` where the num_nodes is how many neurons are in the layer. The final layer of the CNN needs have to have `num_nodes = num_classes`\n",
        "*   Flatten layer: This just converts and n-dimensional tensor into a vector. In this case we use it to present a dense output layer with a vector input\n",
        "\n",
        "\n",
        "In the following way of writing our network, we need to write things in the form:\n",
        "\n",
        "`layer_output = keras.layers.LayerNameHere(arguments_go_here)(layer_input)`\n",
        "\n",
        "For the first CNN we are building, you will hopefully see the following output from the model.summary() command:\n",
        "\n",
        "```\n",
        "Model: \"model\"\n",
        "_________________________________________________________________\n",
        " Layer (type)                 Output Shape              Param #   \n",
        "=================================================================\n",
        " input_1 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
        "                                                                 \n",
        " conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
        "                                                                 \n",
        " max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
        "                                                              \n",
        " dropout (Dropout)            (None, 13, 13, 32)        0         \n",
        "                                                                 \n",
        " flatten (Flatten)            (None, 5408)              0         \n",
        "                                                                 \n",
        " dense (Dense)                (None, 10)                54090     \n",
        "                                                                 \n",
        "=================================================================\n",
        "Total params: 54,410\n",
        "Trainable params: 54,410\n",
        "Non-trainable params: 0\n",
        "_________________________________________________________________\n",
        "```"
      ],
      "metadata": {
        "id": "687KJL-5B_8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define our simplest model first\n",
        "input_layer = keras.layers.Input(x_train[0].shape)\n",
        "x = None(input_layer) # Replace None with a 2D convolution with 32 filters of size (3,3) and relu activation\n",
        "x = None(x) # Replace None with a MaxPooling2D layer to downsample by a factor of 2 in both dimensions\n",
        "x = None(x) # Replace None with a droput layer with a fraction of 0.25\n",
        "x = keras.layers.Flatten()(x)\n",
        "x = None(x) # Replace None with a final dense output layer with num_classes neurons and softmax activation\n",
        "cnn_model = keras.Model(input_layer, x)\n",
        "cnn_model.summary()"
      ],
      "metadata": {
        "id": "GJo_UGUdmJ-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define some of the hyperparameters:\n",
        "\n",
        "\n",
        "*   `batch_size` determines how many images we process in parallel. The larger the number the more computationally efficient the network will be, but also the larger the memory footprint.\n",
        "*   `epochs` is the number of times we iterate over the entire training dataset during the training process\n",
        "*   `learning_rate` determines the step size taken in the gradient descent algorithm\n",
        "\n"
      ],
      "metadata": {
        "id": "V9ksuyKz2Y_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The batch size controls the number of images that are processed simultaneously\n",
        "batch_size = 128\n",
        "# The number of epochs that we want to train the network for\n",
        "epochs = 5\n",
        "# The learning rate (step size in gradient descent)\n",
        "learning_rate = 0.001"
      ],
      "metadata": {
        "id": "PMNZJzKcMdap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to get the network ready for training by defining a loss function and the optimiser that we will use to perform the gradient descent. For this task we need to use categorical crossentropy loss (`keras.losses.categorical_crossentropy`) but other losses are required for other tasks.\n",
        "\n",
        "The code below uses the `Adam` optimiser, but you can investigate using some others to see how they perform. A full list of those available in `keras` can be found here: https://keras.io/api/optimizers/\n",
        "\n"
      ],
      "metadata": {
        "id": "Fyk14B272ELL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss function - for a multi-class classification task we need to\n",
        "# use categorical crossentropy loss\n",
        "loss_function = keras.losses.categorical_crossentropy\n",
        "# The optimiser performs the gradient descent for us. There are a few different\n",
        "# algorithms, but Adam is one of the more popular ones\n",
        "optimiser = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "# Now we compile the model with the loss function and optimiser\n",
        "cnn_model.compile(loss=loss_function, optimizer=optimiser, metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "KumvOGf4y-38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can run our network on whichever data sample we requested. Initially on `mnist` we'll hopefully see that we can reach a very high accuracy.\n",
        "\n",
        "To do this, we use the `fit` function of the `cnn_model` object. The full function definition is given here: https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\n",
        "\n",
        "However, we only need of the arguments here: `fit(x, y, batch_size, epochs, validation_data, verbose)`\n",
        "\n",
        "\n",
        "*   `x` is the training data\n",
        "*   `y` are the training labels\n",
        "*   `batch_size` is the number of images to process together\n",
        "*   `epochs` is the number of epochs\n",
        "*   `validation_data` is the validation sample - we need to use the test sample here. The format here is `(validation_x, validation_y)`.\n",
        "*   `verbose` allows you to get some visual output. I typically set this to `1`."
      ],
      "metadata": {
        "id": "HIYp4y4WRrcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model using the training data with the true target outputs.\n",
        "# Fill in the required arguments using the clues given above\n",
        "cnn_model.fit(x = None, y = None, batch_size = None, epochs = None,\n",
        "              validation_data = (None, None), verbose = 1)\n"
      ],
      "metadata": {
        "id": "Oi9PWqaExzFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you used the `verbose = 1` setting, then you should see the final values of the loss and accuracy from the training and test samples.\n",
        "\n",
        "Since the classification isn't perfect, let's have a look at a few examples of incorrectly classified images. We will run *inference* on the test sample events and look for those that were incorrectly classified. *Inference* corresponds to predicting the class of a given event, and it is also how we run our networks on unseen data. In `keras` we do this with the `predict` member function: https://www.tensorflow.org/api_docs/python/tf/keras/Model#predict\n",
        "\n",
        "In this example we can just use two of the arguments:\n",
        "`predict(x, batch_size)`. This function returns a ten-element array containing the score for each class for each event. It therefore has size `(n_images, 10)`"
      ],
      "metadata": {
        "id": "pEdebQfb--2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a list of incorrect classifications\n",
        "incorrect_indices = []\n",
        "# Let's look at the whole test dataset, but you can reduce this to 1000 or so\n",
        "# if you want run more quickly\n",
        "n_images_to_check = x_test.shape[0]\n",
        "# Use the CNN to predict the classification of the images. It returns an array\n",
        "# containing the 10 class scores for each image. It is best to write this code\n",
        "# using the array notation x[:i] that means use all values of x up until\n",
        "# the index i, such that if you changed the number of images above then it all\n",
        "# still works efficiently\n",
        "raw_predictions = cnn_model.predict(x = None, batch_size = None)\n",
        "for i in range(0,n_images_to_check):\n",
        "  # Remember the raw output from the CNN gives us an array of scores. We want\n",
        "  # to select the highest one as our prediction. We need to do the same thing\n",
        "  # for the truth too since we converted our numbers to a categorical\n",
        "  # representation earlier. We use the np.argmax() function for this\n",
        "  prediction = np.argmax(raw_predictions[i])\n",
        "  truth = np.argmax(y_test[i])\n",
        "  if prediction != truth:\n",
        "    incorrect_indices.append([i,prediction,truth])\n",
        "print('Number of images that were incorrectly classified =',len(incorrect_indices))"
      ],
      "metadata": {
        "id": "Y4Ju8i7pzUAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now you can modify this part to draw different images from the failures list\n",
        "# You can change the value of im to look at different failures\n",
        "im = 0\n",
        "image_to_plot = x_test[incorrect_indices[im][0]]\n",
        "fig, ax = plot.subplots(1, 1)\n",
        "print('Incorrect classification for image',incorrect_indices[im][0],\n",
        "      ': predicted =',incorrect_indices[im][1],\n",
        "      'with true =',incorrect_indices[im][2])\n",
        "ax.imshow(image_to_plot)"
      ],
      "metadata": {
        "id": "ZFP6TCZDz6gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eTY-jYElz9j4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}